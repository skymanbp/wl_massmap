{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a970c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import sep\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================== 1. 滤波器定义 ==================\n",
    "\n",
    "def normalize_filter(weight_array, epsilon=1e-10):\n",
    "    total = np.sum(weight_array)\n",
    "    if total < epsilon:\n",
    "        raise ValueError(\n",
    "            f\"Cannot normalize: filter weights sum to {total}, below threshold {epsilon}.\"\n",
    "        )\n",
    "    return weight_array / total\n",
    "\n",
    "def smooth_center_taper(x, epsilon=1e-5):\n",
    "    return x**2 / (x**2 + epsilon)\n",
    "\n",
    "def schirmer_weight(x):\n",
    "    a, b, c, d, xc = 6., 150., 47., 50., 0.15\n",
    "    Q = 1. / (1. + np.exp(a - b * x) + np.exp(d * x - c))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        fx = np.tanh(x / xc) / (x / xc)\n",
    "        fx[np.isnan(fx)] = 1.0\n",
    "    return Q * fx\n",
    "\n",
    "def gaussian_weight(x):\n",
    "    return np.exp(-x**2 / (2 * 0.3 ** 2))  # std = 0.3\n",
    "\n",
    "def shifted_gaussian_weight(x, mu=0.5, sigma=0.3):\n",
    "    return np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "\n",
    "def top_hat_weight(x, smoothing_width=0.05):\n",
    "    Q = np.ones_like(x)\n",
    "    mask = (x > 1.0) & (x <= 1.0 + smoothing_width)\n",
    "    Q[mask] = 1.0 - (x[mask] - 1.0) / smoothing_width\n",
    "    Q[x > 1.0 + smoothing_width] = 0.0\n",
    "    return Q\n",
    "\n",
    "def nfw_matched_weight(x):\n",
    "    Q = np.zeros_like(x)\n",
    "    eps = 1e-6\n",
    "    x = np.clip(x, eps, None)\n",
    "\n",
    "    mask1 = (x < 1)\n",
    "    mask2 = (x > 1)\n",
    "    mask3 = (x == 1)\n",
    "\n",
    "    Q[mask1] = (2 / (x[mask1]**2 - 1)) * (\n",
    "        1 - (2 / np.sqrt(1 - x[mask1]**2)) *\n",
    "        np.arctanh(np.sqrt((1 - x[mask1])/(1 + x[mask1])))\n",
    "    )\n",
    "    Q[mask2] = (2 / (x[mask2]**2 - 1)) * (\n",
    "        1 - (2 / np.sqrt(x[mask2]**2 - 1)) *\n",
    "        np.arctan(np.sqrt((x[mask2] - 1)/(x[mask2] + 1)))\n",
    "    )\n",
    "    Q[mask3] = 10/3 - 4*np.log(2)\n",
    "    return Q\n",
    "\n",
    "def no_filter_weight(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "filter_bank = {\n",
    "    \"schirmer\": schirmer_weight,\n",
    "    \"gaussian\": gaussian_weight,\n",
    "    \"shifted_gaussian\": shifted_gaussian_weight,\n",
    "    \"tophat\": top_hat_weight,\n",
    "    \"nfw\": nfw_matched_weight,\n",
    "    \"none\": no_filter_weight,\n",
    "}\n",
    "\n",
    "# ================== 2. 配置区 ==================\n",
    "\n",
    "# ---- 目录（按你自己的路径改）----\n",
    "CATALOG_DIR = Path(\"C:/Users/skyma/Desktop/t1/csv\")            # catalog *.csv 所在目录\n",
    "PEAKS_DIR   = Path(\"C:/Users/skyma/Desktop/t1/batch_sep\")      # 对应 *_peaks.csv 所在目录\n",
    "OUT_DIR     = Path(\"C:/Users/skyma/Desktop/t1/blend_out\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"maps\").mkdir(exist_ok=True)\n",
    "(OUT_DIR / \"png\").mkdir(exist_ok=True)\n",
    "\n",
    "# ---- 扫描的参数 ----\n",
    "BIN_SIZE_LIST = [500.0, 1000.0, 1500, 2000.0]      # 原始像素 → mass-map 像素大小\n",
    "REL_RS_LIST   = [5.0, 10.0, 20.0, 30.0, 40.0]     # Rs_rel，Rs_pix = Rs_rel * bin_size_pix\n",
    "FILTER_LIST   = [\"schirmer\", \"gaussian\", \"tophat\", \"nfw\", \"none\"]\n",
    "\n",
    "# aperture 积分最大半径 = RMAX_FACTOR * Rs_pix\n",
    "RMAX_FACTOR = 3.0\n",
    "\n",
    "# mass-map 至少像素数（避免 bin_size 太大变成 1x1 图）\n",
    "MIN_NPIX_X = 16\n",
    "MIN_NPIX_Y = 16\n",
    "\n",
    "# ---- SEP & merge 判定 ----\n",
    "SNR_SCAN_LIST    = [4.0, 3.5, 3.0, 2.5, 2.0, 1.5, 1.0]  # 自动阈值扫描列表（从高到低）\n",
    "MINAREA          = 1                               # mass map 建议用 1\n",
    "MATCH_TOL        = 3.0                             # 真值附近匹配容差（mass-map 像素）\n",
    "MID_FRAC         = 0.3                             # 中点容差 true_sep_small 的比例\n",
    "FALLBACK_SN_MIN  = 1.0\n",
    "FALLBACK_DELTA_MID = 0.6\n",
    "\n",
    "# summary 输出文件\n",
    "OUT_SUMMARY = OUT_DIR / \"results_apmap_sep_binRsFilter.csv\"\n",
    "\n",
    "# ================== 3. 文件 & 真值工具 ==================\n",
    "\n",
    "def base_name_from_catalog(path: Path) -> str:\n",
    "    stem = path.stem\n",
    "    stem = re.sub(r\"\\.r\\d+$\", \"\", stem)\n",
    "    stem = re.sub(r\"_r\\d+$\", \"\", stem)\n",
    "    return stem\n",
    "\n",
    "def load_truth_peaks(peaks_dir: Path, cat_path: Path) -> pd.DataFrame:\n",
    "    base = base_name_from_catalog(cat_path)\n",
    "    peaks_path = peaks_dir / f\"{base}_peaks.csv\"\n",
    "    if not peaks_path.exists():\n",
    "        raise FileNotFoundError(f\"missing peaks file: {peaks_path}\")\n",
    "    return pd.read_csv(peaks_path)\n",
    "\n",
    "def true_sep_from_peaks(peaks_df: pd.DataFrame) -> float:\n",
    "    if len(peaks_df) < 2:\n",
    "        return np.nan\n",
    "    if \"cluster_id\" in peaks_df.columns:\n",
    "        sub = peaks_df[peaks_df[\"cluster_id\"].isin([1, 2])]\n",
    "        if len(sub) < 2:\n",
    "            sub = peaks_df.iloc[:2]\n",
    "    else:\n",
    "        sub = peaks_df.iloc[:2]\n",
    "    a, b = sub.iloc[0], sub.iloc[1]\n",
    "    dx = b[\"x_peak\"] - a[\"x_peak\"]\n",
    "    dy = b[\"y_peak\"] - a[\"y_peak\"]\n",
    "    return float(np.hypot(dx, dy))\n",
    "\n",
    "# ================== 4. KD-tree 加速的 aperture mass ==================\n",
    "\n",
    "def build_aperture_mass_map_from_catalog(\n",
    "    df: pd.DataFrame,\n",
    "    Rs_pix: float,\n",
    "    filter_name: str,\n",
    "    bin_size_pix: float,\n",
    "    x_col: str = \"x\",\n",
    "    y_col: str = \"y\",\n",
    "    e1_col: str = \"e1\",\n",
    "    e2_col: str = \"e2\",\n",
    "    use_center_taper: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    使用 KD-tree 加速的 aperture mass 计算版本。\n",
    "    数学上与原来逐点扫描 (for 每个像素, 对所有星系算 r<=Rmax) 完全等价：\n",
    "      - 同样的 Rs_pix、RMAX_FACTOR\n",
    "      - 同样的滤波器 Q(x=r/Rs)\n",
    "      - 同样的 e_t 定义和归一化\n",
    "\n",
    "    mass map 分辨率由 bin_size_pix 决定：\n",
    "      - 像素宽度 = bin_size_pix（原始 x,y 单位）\n",
    "      - 像素中心 = x_min + (k+0.5)*bin_size_pix\n",
    "    \"\"\"\n",
    "    x  = df[x_col].to_numpy(dtype=np.float64)\n",
    "    y  = df[y_col].to_numpy(dtype=np.float64)\n",
    "    e1 = df[e1_col].to_numpy(dtype=np.float64)\n",
    "    e2 = df[e2_col].to_numpy(dtype=np.float64)\n",
    "\n",
    "    mask = np.isfinite(x) & np.isfinite(y) & np.isfinite(e1) & np.isfinite(e2)\n",
    "    x, y, e1, e2 = x[mask], y[mask], e1[mask], e2[mask]\n",
    "\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"No valid galaxies found in catalog.\")\n",
    "\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "\n",
    "    Lx = x_max - x_min\n",
    "    Ly = y_max - y_min\n",
    "    nx = max(MIN_NPIX_X, int(np.ceil(Lx / bin_size_pix)))\n",
    "    ny = max(MIN_NPIX_Y, int(np.ceil(Ly / bin_size_pix)))\n",
    "\n",
    "    xs_grid = x_min + (np.arange(nx) + 0.5) * bin_size_pix\n",
    "    ys_grid = y_min + (np.arange(ny) + 0.5) * bin_size_pix\n",
    "\n",
    "    gal_pos = np.column_stack((x, y))        # (N_gal, 2)\n",
    "    tree = cKDTree(gal_pos)\n",
    "\n",
    "    Rmax = RMAX_FACTOR * Rs_pix\n",
    "\n",
    "    Xc, Yc = np.meshgrid(xs_grid, ys_grid, indexing=\"xy\")  # (ny, nx)\n",
    "    centers = np.stack([Xc.ravel(), Yc.ravel()], axis=1)   # (N_pix, 2)\n",
    "\n",
    "    neighbors_list = tree.query_ball_point(centers, r=Rmax)\n",
    "\n",
    "    filt_func = filter_bank[filter_name]\n",
    "    map_flat = np.zeros(centers.shape[0], dtype=np.float32)\n",
    "\n",
    "    for idx_center, neigh_idx in enumerate(neighbors_list):\n",
    "        if not neigh_idx:\n",
    "            continue\n",
    "\n",
    "        neigh_idx = np.asarray(neigh_idx, dtype=int)\n",
    "\n",
    "        x0, y0 = centers[idx_center]\n",
    "        dx = x[neigh_idx] - x0\n",
    "        dy = y[neigh_idx] - y0\n",
    "        r  = np.hypot(dx, dy)\n",
    "\n",
    "        m = (r <= Rmax)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "\n",
    "        dxm = dx[m]\n",
    "        dym = dy[m]\n",
    "        rm  = r[m]\n",
    "        em1 = e1[neigh_idx][m]\n",
    "        em2 = e2[neigh_idx][m]\n",
    "\n",
    "        phi = np.arctan2(dym, dxm)\n",
    "        cos2phi = np.cos(2.0 * phi)\n",
    "        sin2phi = np.sin(2.0 * phi)\n",
    "        e_t = -(em1 * cos2phi + em2 * sin2phi)\n",
    "\n",
    "        x_rel = rm / Rs_pix\n",
    "        Q_raw = filt_func(x_rel)\n",
    "        if use_center_taper:\n",
    "            Q_raw = Q_raw * smooth_center_taper(x_rel)\n",
    "\n",
    "        try:\n",
    "            Q = normalize_filter(Q_raw)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        map_flat[idx_center] = np.sum(e_t * Q)\n",
    "\n",
    "    map_ap = map_flat.reshape(ny, nx)\n",
    "    return map_ap, xs_grid, ys_grid\n",
    "\n",
    "def make_sn_map(map_ap: np.ndarray, smooth_sigma_pix: float = 1.0) -> np.ndarray:\n",
    "    img = map_ap.copy()\n",
    "    if smooth_sigma_pix > 0:\n",
    "        img = gaussian_filter(img, sigma=smooth_sigma_pix)\n",
    "    mu = np.mean(img)\n",
    "    sigma = np.std(img)\n",
    "    if sigma <= 0:\n",
    "        sigma = 1.0\n",
    "    return (img - mu) / sigma\n",
    "\n",
    "def compute_truth_small_coords(\n",
    "    peaks_df: pd.DataFrame,\n",
    "    xs_grid: np.ndarray,\n",
    "    ys_grid: np.ndarray,\n",
    "):\n",
    "    \"\"\"\n",
    "    把真值 x_peak,y_peak 映射到 mass map 像素索引 (ix,iy)。\n",
    "    用“找最近的像素中心”实现。\n",
    "    \"\"\"\n",
    "    if len(peaks_df) == 0:\n",
    "        return [], np.nan\n",
    "\n",
    "    if \"cluster_id\" in peaks_df.columns:\n",
    "        sub = peaks_df[peaks_df[\"cluster_id\"].isin([1, 2])]\n",
    "        if len(sub) < 2:\n",
    "            sub = peaks_df.iloc[:2]\n",
    "    else:\n",
    "        sub = peaks_df.iloc[:2]\n",
    "\n",
    "    x_true = sub[\"x_peak\"].to_numpy()\n",
    "    y_true = sub[\"y_peak\"].to_numpy()\n",
    "\n",
    "    ix_list = []\n",
    "    iy_list = []\n",
    "    for xt, yt in zip(x_true, y_true):\n",
    "        ix = int(np.argmin(np.abs(xs_grid - xt)))\n",
    "        iy = int(np.argmin(np.abs(ys_grid - yt)))\n",
    "        ix_list.append(ix)\n",
    "        iy_list.append(iy)\n",
    "\n",
    "    truth_xy_small = list(zip(ix_list, iy_list))\n",
    "\n",
    "    if len(truth_xy_small) >= 2:\n",
    "        dx = ix_list[1] - ix_list[0]\n",
    "        dy = iy_list[1] - iy_list[0]\n",
    "        true_sep_small = float(np.hypot(dx, dy))\n",
    "    else:\n",
    "        true_sep_small = np.nan\n",
    "\n",
    "    return truth_xy_small, true_sep_small\n",
    "\n",
    "# ================== 5. merge 判定（完整版） ==================\n",
    "\n",
    "def classify_merge_custom(\n",
    "    truth_xy_small,\n",
    "    objects,\n",
    "    true_sep_small: float,\n",
    "    match_tol_pix: float = MATCH_TOL,\n",
    "    mid_frac: float = MID_FRAC,\n",
    "):\n",
    "    \"\"\"\n",
    "    根据真值位置 (truth_xy_small) 和 SEP objects 来判断：\n",
    "      - merged: True / False / None\n",
    "      - detection_error: 是否需要 fallback\n",
    "      - pattern: SEP 模式标签（便于 debug / 画图）\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"n_true\": len(truth_xy_small),\n",
    "        \"n_detected_peaks\": 0 if objects is None else len(objects),\n",
    "        \"merged\": None,\n",
    "        \"detection_error\": False,\n",
    "        \"pattern\": \"none\",\n",
    "    }\n",
    "\n",
    "    n_true = len(truth_xy_small)\n",
    "    n_det  = result[\"n_detected_peaks\"]\n",
    "\n",
    "    # 0. 没真值\n",
    "    if n_true == 0:\n",
    "        result[\"detection_error\"] = True\n",
    "        result[\"pattern\"] = \"no_truth\"\n",
    "        return result\n",
    "\n",
    "    # 1. 只有一个真实 cluster：不谈 merge，检测到就当 non-merge\n",
    "    if n_true == 1:\n",
    "        if n_det == 0:\n",
    "            result[\"detection_error\"] = True\n",
    "            result[\"pattern\"] = \"1truth_0det\"\n",
    "            return result\n",
    "        result[\"merged\"] = False\n",
    "        result[\"pattern\"] = \"1truth_ge1det_nonmerge\"\n",
    "        return result\n",
    "\n",
    "    # 2. 真值 >= 2，只用前两个\n",
    "    truths = np.array(truth_xy_small[:2])\n",
    "    t1, t2 = truths[0], truths[1]\n",
    "    mid = 0.5 * (t1 + t2)\n",
    "\n",
    "    if n_det == 0:\n",
    "        result[\"detection_error\"] = True\n",
    "        result[\"pattern\"] = \"0det\"\n",
    "        return result\n",
    "\n",
    "    det_xy = np.vstack((objects[\"x\"], objects[\"y\"])).T\n",
    "\n",
    "    d1   = np.hypot(det_xy[:, 0] - t1[0], det_xy[:, 1] - t1[1])\n",
    "    d2   = np.hypot(det_xy[:, 0] - t2[0], det_xy[:, 1] - t2[1])\n",
    "    dmid = np.hypot(det_xy[:, 0] - mid[0], det_xy[:, 1] - mid[1])\n",
    "\n",
    "    tol_true = match_tol_pix\n",
    "    if np.isfinite(true_sep_small):\n",
    "        tol_mid = max(match_tol_pix, mid_frac * true_sep_small)\n",
    "    else:\n",
    "        tol_mid = match_tol_pix * 1.5\n",
    "\n",
    "    near_t1_idx  = np.where(d1   <= tol_true)[0]\n",
    "    near_t2_idx  = np.where(d2   <= tol_true)[0]\n",
    "    near_mid_idx = np.where(dmid <= tol_mid )[0]\n",
    "\n",
    "    # A. n_det == 1\n",
    "    if n_det == 1:\n",
    "        i0 = 0\n",
    "        cond_mid = (i0 in near_mid_idx)\n",
    "        cond_t1  = (i0 in near_t1_idx)\n",
    "        cond_t2  = (i0 in near_t2_idx)\n",
    "\n",
    "        # 同时满足 mid 和 truth → 优先 merge\n",
    "        if cond_mid and (cond_t1 or cond_t2):\n",
    "            result[\"merged\"] = True\n",
    "            result[\"pattern\"] = \"1det_mid_and_truth_merge\"\n",
    "            return result\n",
    "\n",
    "        # 只满足 mid → merge\n",
    "        if cond_mid:\n",
    "            result[\"merged\"] = True\n",
    "            result[\"pattern\"] = \"1det_mid_merge\"\n",
    "            return result\n",
    "\n",
    "        # 只贴着 truth：有可能是偏心单峰 → 交给 fallback\n",
    "        if cond_t1 or cond_t2:\n",
    "            result[\"detection_error\"] = True\n",
    "            result[\"pattern\"] = \"1det_truth_only_ambiguous\"\n",
    "            return result\n",
    "\n",
    "        # 乱飞 → 错误\n",
    "        result[\"detection_error\"] = True\n",
    "        result[\"pattern\"] = \"1det_far_ambiguous\"\n",
    "        return result\n",
    "\n",
    "    # B. n_det >= 2\n",
    "    has_mid        = len(near_mid_idx) > 0\n",
    "    has_t1         = len(near_t1_idx) > 0\n",
    "    has_t2         = len(near_t2_idx) > 0\n",
    "    has_pair_truth = has_t1 and has_t2\n",
    "\n",
    "    has_merge_pattern    = has_mid\n",
    "    has_nonmerge_pattern = has_pair_truth\n",
    "\n",
    "    # 只有 merge 模式\n",
    "    if has_merge_pattern and not has_nonmerge_pattern:\n",
    "        result[\"merged\"] = True\n",
    "        result[\"pattern\"] = \"multi_merge_only\"\n",
    "        return result\n",
    "\n",
    "    # 只有 non-merge 模式\n",
    "    if has_nonmerge_pattern and not has_merge_pattern:\n",
    "        result[\"merged\"] = False\n",
    "        result[\"pattern\"] = \"multi_nonmerge_only\"\n",
    "        return result\n",
    "\n",
    "    # merge & non-merge 同时出现 → 冲突，交 fallback\n",
    "    if has_merge_pattern and has_nonmerge_pattern:\n",
    "        result[\"detection_error\"] = True\n",
    "        result[\"pattern\"] = \"multi_merge_and_nonmerge_conflict\"\n",
    "        return result\n",
    "\n",
    "    # 没有任何模式\n",
    "    result[\"detection_error\"] = True\n",
    "    result[\"pattern\"] = \"multi_no_pattern\"\n",
    "    return result\n",
    "\n",
    "# ================== 6. fallback：直接看 S/N map ==================\n",
    "\n",
    "def fallback_classify_from_snmap(\n",
    "    sn_map: np.ndarray,\n",
    "    truth_xy_small,\n",
    "    true_sep_small: float,\n",
    "    sn_min: float = FALLBACK_SN_MIN,\n",
    "    delta_mid: float = FALLBACK_DELTA_MID,\n",
    "):\n",
    "    res = {\n",
    "        \"merged_fb\": None,\n",
    "        \"sn1\": np.nan,\n",
    "        \"sn2\": np.nan,\n",
    "        \"sn_mid\": np.nan,\n",
    "    }\n",
    "\n",
    "    if len(truth_xy_small) < 2:\n",
    "        return res\n",
    "\n",
    "    (x1, y1), (x2, y2) = truth_xy_small[0], truth_xy_small[1]\n",
    "    xm = 0.5 * (x1 + x2)\n",
    "    ym = 0.5 * (y1 + y2)\n",
    "\n",
    "    def _sample_sn(x, y):\n",
    "        xi = int(np.clip(round(x), 0, sn_map.shape[1]-1))\n",
    "        yi = int(np.clip(round(y), 0, sn_map.shape[0]-1))\n",
    "        return float(sn_map[yi, xi])\n",
    "\n",
    "    sn1 = _sample_sn(x1, y1)\n",
    "    sn2 = _sample_sn(x2, y2)\n",
    "    sn_mid = _sample_sn(xm, ym)\n",
    "\n",
    "    res[\"sn1\"], res[\"sn2\"], res[\"sn_mid\"] = sn1, sn2, sn_mid\n",
    "\n",
    "    sn_max = max(sn1, sn2)\n",
    "\n",
    "    # 两侧强，中点弱 → non-merge\n",
    "    if (sn1 > sn_min) and (sn2 > sn_min) and (sn_mid < sn_max - delta_mid):\n",
    "        res[\"merged_fb\"] = False\n",
    "        return res\n",
    "\n",
    "    # 中点强，且不比两侧弱 → merge\n",
    "    if (sn_mid > sn_min) and (sn_mid >= sn_max - delta_mid) and (sn_mid > sn_max):\n",
    "        res[\"merged_fb\"] = True\n",
    "        return res\n",
    "\n",
    "    return res\n",
    "\n",
    "# ================== 7. 自动阈值扫描 ==================\n",
    "\n",
    "def run_sep_auto_threshold(\n",
    "    sn_map: np.ndarray,\n",
    "    truth_xy_small,\n",
    "    true_sep_small: float,\n",
    "    snr_list=None,\n",
    "    minarea: int = MINAREA,\n",
    "):\n",
    "    \"\"\"\n",
    "    对同一张 sn_map 自动扫描多个 S/N 阈值。\n",
    "    优先选择能给出明确 merge/non-merge（detection_error=False）的最高阈值。\n",
    "    若所有阈值都 detection_error，则返回最后一次结果，交给 fallback。\n",
    "    \"\"\"\n",
    "    if snr_list is None:\n",
    "        snr_list = SNR_SCAN_LIST\n",
    "\n",
    "    data = np.ascontiguousarray(sn_map.astype(np.float32))\n",
    "    bkg_val = np.median(data)\n",
    "    rms = np.std(data)\n",
    "    if rms <= 0:\n",
    "        rms = 1.0\n",
    "    data_sub = data - bkg_val\n",
    "\n",
    "    sep.set_extract_pixstack(5_000_000)\n",
    "\n",
    "    best_objects = None\n",
    "    best_snr = None\n",
    "    best_cls = None\n",
    "\n",
    "    last_objects = None\n",
    "    last_snr = None\n",
    "    last_cls = None\n",
    "\n",
    "    for snr in snr_list:\n",
    "        thresh = snr * rms\n",
    "        objs = sep.extract(data_sub, thresh=thresh, err=rms, minarea=minarea)\n",
    "\n",
    "        cls = classify_merge_custom(\n",
    "            truth_xy_small=truth_xy_small,\n",
    "            objects=objs,\n",
    "            true_sep_small=true_sep_small,\n",
    "            match_tol_pix=MATCH_TOL,\n",
    "            mid_frac=MID_FRAC,\n",
    "        )\n",
    "\n",
    "        last_objects = objs\n",
    "        last_snr = snr\n",
    "        last_cls = cls\n",
    "\n",
    "        if not cls[\"detection_error\"]:\n",
    "            best_objects = objs\n",
    "            best_snr = snr\n",
    "            best_cls = cls\n",
    "            break\n",
    "\n",
    "    if best_cls is None:\n",
    "        best_objects = last_objects\n",
    "        best_snr = last_snr\n",
    "        if last_cls is not None:\n",
    "            best_cls = last_cls\n",
    "        else:\n",
    "            best_cls = {\n",
    "                \"n_true\": len(truth_xy_small),\n",
    "                \"n_detected_peaks\": 0 if best_objects is None else len(best_objects),\n",
    "                \"merged\": None,\n",
    "                \"detection_error\": True,\n",
    "                \"pattern\": \"auto_snr_all_error\",\n",
    "            }\n",
    "\n",
    "    return best_objects, best_snr, best_cls\n",
    "\n",
    "# ================== 8. PNG 诊断图 ==================\n",
    "\n",
    "def plot_diagnostic_map(\n",
    "    sn_map: np.ndarray,\n",
    "    objects,\n",
    "    truth_xy_small,\n",
    "    out_path: Path,\n",
    "    title_main: str = \"\",\n",
    "    merged_final=None,\n",
    "    merged_sep_rule=None,\n",
    "    detection_error_sep=False,\n",
    "    used_fallback=False,\n",
    "    snr_used=None,\n",
    "    n_sep_peaks=None,\n",
    "    Rs_rel=None,\n",
    "    Rs_pix=None,\n",
    "    bin_size_pix=None,\n",
    "    filter_name=None,\n",
    "    pattern_sep_rule: str = None,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(sn_map, origin=\"lower\", cmap=\"inferno\")\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"M_ap / sigma\")\n",
    "\n",
    "    if objects is not None and len(objects) > 0:\n",
    "        ax.scatter(\n",
    "            objects[\"x\"], objects[\"y\"],\n",
    "            s=60, facecolors=\"none\", edgecolors=\"cyan\",\n",
    "            linewidths=1.5, label=\"SEP peaks\"\n",
    "        )\n",
    "\n",
    "    if truth_xy_small:\n",
    "        xt = [p[0] for p in truth_xy_small]\n",
    "        yt = [p[1] for p in truth_xy_small]\n",
    "        ax.scatter(\n",
    "            xt, yt,\n",
    "            s=80, marker=\"x\", color=\"red\",\n",
    "            linewidths=2.0, label=\"truth\"\n",
    "        )\n",
    "\n",
    "    def _fmt(val):\n",
    "        if val is True:\n",
    "            return \"MERGE\"\n",
    "        if val is False:\n",
    "            return \"NON-MERGE\"\n",
    "        return \"None\"\n",
    "\n",
    "    text_lines = [\n",
    "        f\"merged_final = {_fmt(merged_final)}\",\n",
    "        f\"merged_sep_rule = {_fmt(merged_sep_rule)}\",\n",
    "        f\"pattern_sep_rule = {pattern_sep_rule}\" if pattern_sep_rule is not None else \"\",\n",
    "        f\"SEP peaks = {n_sep_peaks}\",\n",
    "        f\"detection_error_sep = {detection_error_sep}\",\n",
    "        f\"used_fallback = {used_fallback}\",\n",
    "        f\"snr_used = {snr_used:.2f}\" if snr_used is not None else \"\",\n",
    "        f\"bin_size = {bin_size_pix} pix\" if bin_size_pix is not None else \"\",\n",
    "        f\"Rs_rel = {Rs_rel}, Rs_pix = {Rs_pix:.0f}\",\n",
    "        f\"filter = {filter_name}\" if filter_name is not None else \"\",\n",
    "    ]\n",
    "    text_lines = [t for t in text_lines if t]\n",
    "\n",
    "    ax.text(\n",
    "        0.02, 0.98,\n",
    "        \"\\n\".join(text_lines),\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=8, color=\"white\",\n",
    "        ha=\"left\", va=\"top\",\n",
    "        bbox=dict(facecolor=\"black\", alpha=0.35, edgecolor=\"none\", pad=6)\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"mass-map pixel x\")\n",
    "    ax.set_ylabel(\"mass-map pixel y\")\n",
    "    ax.set_title(title_main, fontsize=10)\n",
    "\n",
    "    if (objects is not None and len(objects) > 0) or truth_xy_small:\n",
    "        ax.legend(loc=\"upper right\", fontsize=7)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ================== 9. Checkpoint 系统 ==================\n",
    "\n",
    "CKPT_PATH = OUT_DIR / \"checkpoint_done.csv\"\n",
    "\n",
    "if CKPT_PATH.exists():\n",
    "    checkpoint_df = pd.read_csv(CKPT_PATH)\n",
    "else:\n",
    "    checkpoint_df = pd.DataFrame(\n",
    "        columns=[\"catalog\", \"bin_size\", \"Rs_rel\", \"filter\"]\n",
    "    )\n",
    "\n",
    "def is_done(cat_name, bin_size, Rs_rel, filter_name):\n",
    "    if checkpoint_df.empty:\n",
    "        return False\n",
    "    mask = (\n",
    "        (checkpoint_df[\"catalog\"] == cat_name) &\n",
    "        (checkpoint_df[\"bin_size\"] == bin_size) &\n",
    "        (checkpoint_df[\"Rs_rel\"] == Rs_rel) &\n",
    "        (checkpoint_df[\"filter\"] == filter_name)\n",
    "    )\n",
    "    return bool(mask.any())\n",
    "\n",
    "def save_checkpoint(cat_name, bin_size, Rs_rel, filter_name):\n",
    "    global checkpoint_df\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"catalog\": cat_name,\n",
    "        \"bin_size\": bin_size,\n",
    "        \"Rs_rel\": Rs_rel,\n",
    "        \"filter\": filter_name\n",
    "    }])\n",
    "    checkpoint_df = pd.concat([checkpoint_df, new_row], ignore_index=True)\n",
    "    checkpoint_df.to_csv(CKPT_PATH, index=False)\n",
    "\n",
    "# ================== 10. merge probability heatmap ==================\n",
    "\n",
    "def plot_merge_probability_heatmaps(summary_df: pd.DataFrame, out_dir: Path):\n",
    "    \"\"\"\n",
    "    对每个 filter 画一张 2D heatmap:\n",
    "      x 轴 = Rs_rel\n",
    "      y 轴 = bin_size_pix\n",
    "      color = P(merged_final=True)\n",
    "    \"\"\"\n",
    "    df = summary_df.copy()\n",
    "    df = df[df[\"merged_final\"].isin([True, False])]\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No valid merged_final data to make heatmaps.\")\n",
    "        return\n",
    "\n",
    "    filters = sorted(df[\"filter\"].unique())\n",
    "\n",
    "    for f in filters:\n",
    "        sub = df[df[\"filter\"] == f].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        sub[\"is_merged\"] = sub[\"merged_final\"].astype(int)\n",
    "\n",
    "        grouped = sub.groupby([\"bin_size_pix\", \"Rs_rel\"])[\"is_merged\"].mean().reset_index()\n",
    "\n",
    "        bin_sizes = sorted(grouped[\"bin_size_pix\"].unique())\n",
    "        Rs_vals   = sorted(grouped[\"Rs_rel\"].unique())\n",
    "\n",
    "        mat = np.full((len(bin_sizes), len(Rs_vals)), np.nan, dtype=float)\n",
    "        for i, b in enumerate(bin_sizes):\n",
    "            for j, r in enumerate(Rs_vals):\n",
    "                tmp = grouped[(grouped[\"bin_size_pix\"] == b) & (grouped[\"Rs_rel\"] == r)]\n",
    "                if not tmp.empty:\n",
    "                    mat[i, j] = tmp[\"is_merged\"].iloc[0]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        im = ax.imshow(\n",
    "            mat,\n",
    "            origin=\"lower\",\n",
    "            aspect=\"auto\",\n",
    "            extent=[\n",
    "                min(Rs_vals)-0.5, max(Rs_vals)+0.5,\n",
    "                min(bin_sizes)-0.5, max(bin_sizes)+0.5\n",
    "            ],\n",
    "            vmin=0.0, vmax=1.0,\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "        cbar.set_label(\"P(merged_final = True)\")\n",
    "\n",
    "        ax.set_xlabel(\"Rs_rel\")\n",
    "        ax.set_ylabel(\"bin_size_pix\")\n",
    "        ax.set_title(f\"Merge probability heatmap (filter={f})\")\n",
    "\n",
    "        ax.set_xticks(Rs_vals)\n",
    "        ax.set_yticks(bin_sizes)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        out_png = out_dir / f\"merge_prob_heatmap_filter_{f}.png\"\n",
    "        fig.savefig(out_png, dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved heatmap for filter={f} to {out_png}\")\n",
    "\n",
    "# ================== 11. 主循环（实时保存 summary CSV） ==================\n",
    "\n",
    "def append_row_to_summary(row: dict):\n",
    "    \"\"\"\n",
    "    立刻把一行结果 append 到 OUT_SUMMARY，对中途中断友好。\n",
    "    \"\"\"\n",
    "    df_row = pd.DataFrame([row])\n",
    "    # header 只在文件首次创建时写\n",
    "    df_row.to_csv(\n",
    "        OUT_SUMMARY,\n",
    "        mode=\"a\",\n",
    "        header=not OUT_SUMMARY.exists(),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "def process_all_catalogs():\n",
    "    cat_files = sorted(CATALOG_DIR.glob(\"*.csv\"))\n",
    "    cat_files = [f for f in cat_files if not f.name.endswith(\"_peaks.csv\")]\n",
    "\n",
    "    print(f\"Found {len(cat_files)} catalog files\")\n",
    "\n",
    "    for i, cat_path in enumerate(cat_files, start=1):\n",
    "        print(f\"\\n===== [{i}/{len(cat_files)}] {cat_path.name} =====\")\n",
    "        df = pd.read_csv(cat_path)\n",
    "\n",
    "        try:\n",
    "            peaks_df = load_truth_peaks(PEAKS_DIR, cat_path)\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"  WARNING:\", e)\n",
    "            peaks_df = pd.DataFrame()\n",
    "\n",
    "        true_sep_orig = true_sep_from_peaks(peaks_df)\n",
    "        print(f\"  true sep (orig pix) = {true_sep_orig}\")\n",
    "\n",
    "        for bin_size_pix in BIN_SIZE_LIST:\n",
    "            print(f\"  === bin_size = {bin_size_pix} pix ===\")\n",
    "\n",
    "            for rel_rs in REL_RS_LIST:\n",
    "                Rs_pix = rel_rs * bin_size_pix\n",
    "                print(f\"    -- Rs_rel={rel_rs:.1f}, Rs_pix={Rs_pix:.1f} --\")\n",
    "\n",
    "                for filter_name in FILTER_LIST:\n",
    "\n",
    "                    if is_done(cat_path.name, bin_size_pix, rel_rs, filter_name):\n",
    "                        print(f\"       * {filter_name}: SKIP (checkpoint)\")\n",
    "                        continue\n",
    "\n",
    "                    print(f\"       * filter = {filter_name}\")\n",
    "\n",
    "                    # 1) mass map\n",
    "                    map_ap, xs_grid, ys_grid = build_aperture_mass_map_from_catalog(\n",
    "                        df,\n",
    "                        Rs_pix=Rs_pix,\n",
    "                        filter_name=filter_name,\n",
    "                        bin_size_pix=bin_size_pix,\n",
    "                        x_col=\"x\",\n",
    "                        y_col=\"y\",\n",
    "                        e1_col=\"e1\",\n",
    "                        e2_col=\"e2\",\n",
    "                        use_center_taper=False,\n",
    "                    )\n",
    "\n",
    "                    # 2) S/N map\n",
    "                    sn_map = make_sn_map(map_ap, smooth_sigma_pix=1.0)\n",
    "\n",
    "                    # 3) 真值映射到 mass-map 像素坐标\n",
    "                    truth_xy_small, true_sep_small = compute_truth_small_coords(\n",
    "                        peaks_df,\n",
    "                        xs_grid,\n",
    "                        ys_grid,\n",
    "                    )\n",
    "\n",
    "                    # 4) 自动阈值扫描 + SEP + 初步分类\n",
    "                    objects, snr_used, cls = run_sep_auto_threshold(\n",
    "                        sn_map,\n",
    "                        truth_xy_small,\n",
    "                        true_sep_small,\n",
    "                        snr_list=SNR_SCAN_LIST,\n",
    "                        minarea=MINAREA,\n",
    "                    )\n",
    "\n",
    "                    n_sep_peaks = 0 if objects is None else len(objects)\n",
    "\n",
    "                    merged_final = cls[\"merged\"]\n",
    "                    used_fallback = False\n",
    "\n",
    "                    # === 新逻辑：multi_merge_only 也强制走 fallback ===\n",
    "                    force_fallback = False\n",
    "                    if (cls[\"merged\"] is True) and (cls[\"pattern\"] == \"multi_merge_only\"):\n",
    "                        force_fallback = True\n",
    "\n",
    "                    # 5) 如果仍 detection_error 或者需要强制 fallback，则 fallback\n",
    "                    if cls[\"detection_error\"] or force_fallback:\n",
    "                        fb = fallback_classify_from_snmap(\n",
    "                            sn_map,\n",
    "                            truth_xy_small,\n",
    "                            true_sep_small=true_sep_small,\n",
    "                            sn_min=FALLBACK_SN_MIN,\n",
    "                            delta_mid=FALLBACK_DELTA_MID,\n",
    "                        )\n",
    "                        used_fallback = True\n",
    "                        if fb[\"merged_fb\"] is not None:\n",
    "                            # fallback 有明确结论 → 用 fallback 结果\n",
    "                            merged_final = fb[\"merged_fb\"]\n",
    "                            print(f\"         fallback: sn1={fb['sn1']:.2f}, \"\n",
    "                                f\"sn2={fb['sn2']:.2f}, sn_mid={fb['sn_mid']:.2f} \"\n",
    "                                f\"-> merged={merged_final}\")\n",
    "                        else:\n",
    "                            # fallback 也判断不出 → 最终当作 None\n",
    "                            merged_final = None\n",
    "                            print(f\"         fallback inconclusive \"\n",
    "                                f\"(sn1={fb['sn1']:.2f}, sn2={fb['sn2']:.2f}, sn_mid={fb['sn_mid']:.2f}), \"\n",
    "                                f\"set merged_final=None\")\n",
    "\n",
    "                    # 6) 保存 PNG\n",
    "                    png_name = (f\"{cat_path.stem}_bin{int(bin_size_pix)}\"\n",
    "                                f\"_Rsrel{int(rel_rs)}_{filter_name}.png\")\n",
    "                    png_path = OUT_DIR / \"png\" / png_name\n",
    "                    title = (f\"{cat_path.name}\\n\"\n",
    "                             f\"bin={bin_size_pix} pix, Rs_rel={rel_rs}, \"\n",
    "                             f\"Rs_pix={Rs_pix:.0f}, filter={filter_name}\")\n",
    "\n",
    "                    plot_diagnostic_map(\n",
    "                        sn_map,\n",
    "                        objects,\n",
    "                        truth_xy_small,\n",
    "                        png_path,\n",
    "                        title_main=title,\n",
    "                        merged_final=merged_final,\n",
    "                        merged_sep_rule=cls[\"merged\"],\n",
    "                        detection_error_sep=cls[\"detection_error\"],\n",
    "                        used_fallback=used_fallback,\n",
    "                        snr_used=snr_used,\n",
    "                        n_sep_peaks=n_sep_peaks,\n",
    "                        Rs_rel=rel_rs,\n",
    "                        Rs_pix=Rs_pix,\n",
    "                        bin_size_pix=bin_size_pix,\n",
    "                        filter_name=filter_name,\n",
    "                        pattern_sep_rule=cls[\"pattern\"],\n",
    "                    )\n",
    "\n",
    "                    # 7) 记录结果（实时 append 到 summary CSV）\n",
    "                    row = {\n",
    "                        \"file\": cat_path.name,\n",
    "                        \"base_name\": base_name_from_catalog(cat_path),\n",
    "                        \"bin_size_pix\": bin_size_pix,\n",
    "                        \"Rs_rel\": rel_rs,\n",
    "                        \"Rs_pix\": Rs_pix,\n",
    "                        \"filter\": filter_name,\n",
    "                        \"true_sep_pix_orig\": true_sep_orig,\n",
    "                        \"true_sep_pix_small\": true_sep_small,\n",
    "                        \"n_true\": cls[\"n_true\"],\n",
    "                        \"n_detected_peaks\": cls[\"n_detected_peaks\"],\n",
    "                        \"n_sep_peaks\": n_sep_peaks,\n",
    "                        \"snr_used\": snr_used,\n",
    "                        \"merged_sep_rule\": cls[\"merged\"],\n",
    "                        \"merged_final\": merged_final,\n",
    "                        \"detection_error_sep\": cls[\"detection_error\"],\n",
    "                        \"used_fallback\": used_fallback,\n",
    "                        \"pattern_sep_rule\": cls[\"pattern\"],\n",
    "                    }\n",
    "                    for k, (xt, yt) in enumerate(truth_xy_small[:2], start=1):\n",
    "                        row[f\"c{k}_x_true_small\"] = xt\n",
    "                        row[f\"c{k}_y_true_small\"] = yt\n",
    "\n",
    "                    append_row_to_summary(row)\n",
    "\n",
    "                    # 8) 写 checkpoint\n",
    "                    save_checkpoint(cat_path.name, bin_size_pix, rel_rs, filter_name)\n",
    "\n",
    "    print(\"\\nAll combos done.\")\n",
    "    if OUT_SUMMARY.exists():\n",
    "        summary = pd.read_csv(OUT_SUMMARY)\n",
    "        print(\"Summary CSV:\", OUT_SUMMARY, \"rows =\", len(summary))\n",
    "        plot_merge_probability_heatmaps(summary, OUT_DIR)\n",
    "    else:\n",
    "        print(\"No summary CSV found, nothing to plot.\")\n",
    "\n",
    "# ================== 12. 入口 ==================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_catalogs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
